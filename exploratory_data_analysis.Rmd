---
title: "Exploratory data analysis"
output:
  pdf_document:
    number_sections: yes
  html_notebook:
    number_sections: yes
  html_document:
    number_sections: yes
---


```{r global_options, include=FALSE}
#knitr::opts_chunk$set(fig.width=12, fig.height=8, echo=FALSE, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)

```

# Introduction

The purpose of this document is to show the exploratory analysis and text mining done isofar in the text corpus and to preview some future directions in the construction of the text prediction algorithm.

The source for this document and other project code can be found in [github.com/enrique-repulles/predictive-text].

# Data sources

The text corpus was obtained from 

["https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"] 

An script for download this file  can be found in the project github page.

For the "profanity filtering" cleaning, we've used a offensive words list from  [www.frontgatemedia.com/new/wp-content/uploads/2014/03/Terms-to-Block.csv]



# Data cleaning and preprocessing




For deciding where to split a text into sentences, in the variable endphrase.symbols we store some charactes that define the end of a sentence ('.',';'...)

The *cleanSentences* function removes weird characters, and unify the format (lower case words, single spaces between words...).

Also, any offensive word ("profanity filtering") are removed.

About stop words (like 'the', 'a', 'an'...): I've decided to keep them. In text clasification they are usually removed, but I guess that for prediction they will be usefull.



```{r}

## Preliminaries 

# The libraries needed to run the code
library("NLP")
library("tm")
library("ggplot2")

set.seed(1234)
#sample.size=.01
sample.size=.001


# Read the list of offensive words 
offensive.words<-readLines("raw_data/Terms-to-Block.csv")
offensive.words<-offensive.words[5:length(offensive.words)]
offensive.words<-gsub("[\"]","",offensive.words)
offensive.words<-gsub("[,]","",offensive.words)


# The *loadFile* function loads the file and extract a random subset of lines.
loadFile<-function(myfilename, sample.proportion)
{
  path<-paste0("./raw_data/final/en_US/",myfilename)
  raw.data = readLines(path,warn = FALSE,skipNul = TRUE);
  sample<-1==rbinom(n=length(raw.data), p=sample.proportion,size=1)
  raw.data[sample]
}

extractSentences<-function (text)
{
  endsentence.symbols <- "[.;()!?:]"
  sentences<-unlist(strsplit(text, split=endsentence.symbols))
  sentences<-sentences[nchar(sentences)>5]
}



cleanSentences <-function(rawSentences)
{
  cleanSentences<-rawSentences
  cleanSentences<-tolower(cleanSentences)
  cleanSentences<-stripWhitespace(cleanSentences)
  cleanSentences<-removeNumbers(cleanSentences)
  cleanSentences<-removePunctuation(cleanSentences, preserve_intra_word_dashes = TRUE)
  # english_stop_words<-stopwords()
  # cleanSentences<-removeWords(cleanSentences,english_stop_words)
  cleanSentences<-removeWords(cleanSentences,offensive.words)
  cleanSentences<-trimws(stripWhitespace(cleanSentences))
}




# We include this function for testing purposes
testCleaning <- function(sentences)
{
  data.frame(raw=sentences, clean=cleanSentences(sentences))
}



# This function get a sentence and return a list of tokens. The "ngram.size" parameter indicates the size of the tokens, so for ngram.size=1, the function returns a list of words, for ngram.size=2 returns a list of bigrams, ngram.size=3 returns a list of trigrams, etc 


tokenize <- function (sentence, ngram.size)
{
  words <- unlist(strsplit(sentence," "))
  result.size <- length(words) - ngram.size + 1 
  result<- character(result.size)
  for(i in 1:result.size)
  {
    ngram <- paste(words[i:(i+ngram.size-1)], collapse=" ")
    result[i]<-ngram
  }
  result
}


# File tokenization
# The *tokenizeFile* process a whole file: loads the file, performs cleaning and tokenization and returns a list of tokens.
# The tokens returned are ngrams of several sizes, from 1-grams (words) to 4-grams.
# The result is organize in a data frame, whith one row for each token and the following fields: 
# - filename: the filethe token cames from.
# - ngram.size: 1,2,3,4 indicating if the token is a 1-gram(single word), 2-gram, 3-gram, 4-gram
# - token:  the text of the token


# File to extract counts and statistics about a file 
countFileData <- function(filename)
{
  text<-loadFile(filename, 1)
  totalrows<-length(text)
  text<-loadFile(filename, sample.size)
  samplerows<-length(text)

  rawSentences<-extractSentences(text)
  samplesentences<-length(rawSentences)
  
  data.frame(filename=filename, totalrows,samplerows,samplesentences )
}



tokenizeFile <- function(filename)
{
  text<-loadFile(filename, sample.size)
  rawSentences<-extractSentences(text)
  cleanSentences<-cleanSentences(rawSentences)
  
  ngrams1<-data.frame(token=tokenize(cleanSentences,1)) # Single words
  ngrams2<-data.frame(token=tokenize(cleanSentences,2)) # Bigrams
  ngrams3<-data.frame(token=tokenize(cleanSentences,3)) # trigrams
  ngrams4<-data.frame(token=tokenize(cleanSentences,4)) # 4-grams
  
  # Organize results in a single dataframe
  
  ngrams1$ngram.size<-1
  ngrams2$ngram.size<-2
  ngrams3$ngram.size<-3
  ngrams4$ngram.size<-4
  
  result <- rbind(ngrams1,ngrams2,ngrams3,ngrams4)
  result$origin<-filename
  result
}



```



[File Count]


```{r}
# Using the functions described in the previous section, we process the three 
# files from the corpus. The result is stored in a file for the following analysis step.

# Basic counts for statistics


fileCounts<-rbind(countFileData("en_US.blogs.txt"),
                  countFileData("en_US.news.txt"),
                  countFileData("en_US.twitter.txt"))


fileCounts

```



plot: barra con el tamaÃ±o de los tres ficheros
```{r}

ggplot(fileCounts, aes(x=filename,weight=totalrows)) + geom_bar()

```



plot: barra con dos barras por fichero: filas y frases


```{r}


plotdata1<-data.frame(filename=fileCounts$filename, weight=fileCounts$samplerows) 
plotdata1$type <- "sample rows"
plotdata2<-data.frame(filename=fileCounts$filename, weight=fileCounts$samplesentences) 
plotdata2$type <- "sample sentences"

ggplot(rbind(plotdata1,plotdata2), aes(x=filename,weight=weight, fill=type)) + geom_bar(position="dodge")

```





```{r}

# Process execution

file1Processing<-tokenizeFile("en_US.blogs.txt")
file2Processing<-tokenizeFile("en_US.news.txt")
file3Processing<-tokenizeFile("en_US.twitter.txt")

tokens<-rbind(file1Processing,file2Processing,file3Processing)
tokens$token <- as.character(tokens$token)

#cleanup
rm(file1Processing,file2Processing,file3Processing)
```



[Counts de palabras y ngram size]



```{r}
# word counts


wordCounts<-aggregate(tokens$ngram.size, by=list(filename=tokens$origin, ngram.size=tokens$ngram.size), length)
wordCounts

```


plot:

barras con words por fichero




```{r}
ggplot(wordCounts, aes(x=filename,weight=x)) + geom_bar()
```



# Word frequency analysis

(indicar que se juntan  los 3 ficheros)



```{r}

# calculations 

frecuency<-function(ngram.size)
{
  
  df<-as.data.frame(table(tokens[tokens$ngram.size==ngram.size,]$token))
  df$ngram.size<-ngram.size
  data.frame(ngram.size=df$ngram.size,token=df$Var1,freq=df$Freq )
}


word.frequencies<-frecuency(1)
summary(word.frequencies$freq)
bigram.frequencies<-frecuency(2)
summary(bigram.frequencies$freq)
trigram.frequencies<-frecuency(3)
summary(trigram.frequencies$freq)
quatrigram.frequencies<-frecuency(4)
summary(quatrigram.frequencies$freq)


total.frequencies<-rbind(word.frequencies,bigram.frequencies,trigram.frequencies,quatrigram.frequencies)



```




Que palabras/grams son mas frecuentes

```{r}


word.frequencies[word.frequencies$freq>600,c("token","freq")]

bigram.frequencies[bigram.frequencies$freq>80,c("token","freq")]

trigram.frequencies[trigram.frequencies$freq>11,c("token","freq")]

quatrigram.frequencies[quatrigram.frequencies$freq>3,c("token","freq")]



```



Expllicar antes como uso ngram.size


Historama de frecuencias

```{r}
#calculos previos


calculate.freq.distribution <- function (ngram.size)
{
  result<-data.frame(table(total.frequencies$freq))
  names(result)<-c("freq","n")
  result$freq<-as.numeric(result$freq)
  result$ngram.size<-ngram.size
  result
}

frequency.distribution<-rbind(calculate.freq.distribution(1),
                              calculate.freq.distribution(2),
                              calculate.freq.distribution(3),
                              calculate.freq.distribution(4))



```


Distribucion de palabras


```{r}
ggplot(frequency.distribution[frequency.distribution$ngram==1,], aes(x=freq,y=n)) + geom_col() 

```


No se ve nada,probamos con logaritmos


```{r}
ggplot(frequency.distribution[frequency.distribution$ngram==1,], aes(x=freq,y=log(n))) + geom_col() 

```


Distribucion de ngramas
```{r}

# Distribucion de palabras


#Distribution of ngrams

ggplot(frequency.distribution[frequency.distribution$ngram!=1 & frequency.distribution$freq>5, ], aes(x=freq,y=log(n))) + geom_col() +  
  facet_grid(.~ngram.size) 




```


# Conclusiones


# Future directions 


# TODO 

- labels in plots


