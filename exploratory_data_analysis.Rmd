---
title: "Exploratory data analysis"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---


# Introduction

The purpose of this document is to show the exploratory analysis and text mining done isofar in the text corpus and to preview some future directions in the construction of the text prediction algorithm.

The source for this document and other project code can be found in [github.com/enrique-repulles/predictive-text].

# Data sources

The text corpus was obtained from 

["https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"] 

An script for download this file  can be found in the project github page.

For the "profanity filtering" cleaning, we've used a offensive words list from  [www.frontgatemedia.com/new/wp-content/uploads/2014/03/Terms-to-Block.csv]



# Data cleaning and preprocessing







For deciding where to split a text into sentences, in the variable endphrase.symbols we store some charactes that define the end of a sentence ('.',';'...)

The *cleanSentences* function removes weird characters, and unify the format (lower case words, single spaces between words...).

Also, any offensive word ("profanity filtering") are removed.

About stop words (like 'the', 'a', 'an'...): I've decided to keep them. In text clasification they are usually removed, but I guess that for prediction they will be usefull.



```{r, include=FALSE}

## Preliminaries 

# The libraries needed to run the code
library("NLP")
library("tm")
library("ggplot2")

set.seed(1234)
#sample.size=.01
sample.size=.001


# Read the list of offensive words 
offensive.words<-readLines("raw_data/Terms-to-Block.csv")
offensive.words<-offensive.words[5:length(offensive.words)]
offensive.words<-gsub("[\"]","",offensive.words)
offensive.words<-gsub("[,]","",offensive.words)


# The *loadFile* function loads the file and extract a random subset of lines.
loadFile<-function(myfilename, sample.proportion)
{
  path<-paste0("./raw_data/final/en_US/",myfilename)
  raw.data = readLines(path,warn = FALSE,skipNul = TRUE);
  sample<-1==rbinom(n=length(raw.data), p=sample.proportion,size=1)
  raw.data[sample]
}

extractSentences<-function (text)
{
  endsentence.symbols <- "[.;()!?:]"
  sentences<-unlist(strsplit(text, split=endsentence.symbols))
  sentences<-sentences[nchar(sentences)>5]
}



cleanSentences <-function(rawSentences)
{
  cleanSentences<-rawSentences
  cleanSentences<-tolower(cleanSentences)
  cleanSentences<-stripWhitespace(cleanSentences)
  cleanSentences<-removeNumbers(cleanSentences)
  cleanSentences<-removePunctuation(cleanSentences, preserve_intra_word_dashes = TRUE)
  # english_stop_words<-stopwords()
  # cleanSentences<-removeWords(cleanSentences,english_stop_words)
  cleanSentences<-removeWords(cleanSentences,offensive.words)
  cleanSentences<-trimws(stripWhitespace(cleanSentences))
}




# We include this function for testing purposes
testCleaning <- function(sentences)
{
  data.frame(raw=sentences, clean=cleanSentences(sentences))
}



# This function get a sentence and return a list of tokens. The "ngram.size" parameter indicates the size of the tokens, so for ngram.size=1, the function returns a list of words, for ngram.size=2 returns a list of bigrams, ngram.size=3 returns a list of trigrams, etc 


tokenize <- function (sentence, ngram.size)
{
  words <- unlist(strsplit(sentence," "))
  result.size <- length(words) - ngram.size + 1 
  result<- character(result.size)
  for(i in 1:result.size)
  {
    ngram <- paste(words[i:(i+ngram.size-1)], collapse=" ")
    result[i]<-ngram
  }
  result
}


# File tokenization
# The *tokenizeFile* process a whole file: loads the file, performs cleaning and tokenization and returns a list of tokens.
# The tokens returned are ngrams of several sizes, from 1-grams (words) to 4-grams.
# The result is organize in a data frame, whith one row for each token and the following fields: 
# - filename: the filethe token cames from.
# - ngram.size: 1,2,3,4 indicating if the token is a 1-gram(single word), 2-gram, 3-gram, 4-gram
# - token:  the text of the token


# File to extract counts and statistics about a file 
countFileData <- function(filename)
{
  text<-loadFile(filename, 1)
  totalrows<-length(text)
  text<-loadFile(filename, sample.size)
  samplerows<-length(text)

  rawSentences<-extractSentences(text)
  samplesentences<-length(rawSentences)
  
  data.frame(filename=filename, totalrows,samplerows,samplesentences )
}



tokenizeFile <- function(filename)
{
  text<-loadFile(filename, sample.size)
  rawSentences<-extractSentences(text)
  cleanSentences<-cleanSentences(rawSentences)
  
  ngrams1<-data.frame(token=tokenize(cleanSentences,1)) # Single words
  ngrams2<-data.frame(token=tokenize(cleanSentences,2)) # Bigrams
  ngrams3<-data.frame(token=tokenize(cleanSentences,3)) # trigrams
  ngrams4<-data.frame(token=tokenize(cleanSentences,4)) # 4-grams
  
  # Organize results in a single dataframe
  
  ngrams1$ngram.size<-1
  ngrams2$ngram.size<-2
  ngrams3$ngram.size<-3
  ngrams4$ngram.size<-4
  
  result <- rbind(ngrams1,ngrams2,ngrams3,ngrams4)
  result$origin<-filename
  result
}



```



```{r}
# Using the functions described in the previous section, we process the three 
# files from the corpus. The result is stored in a file for the following analysis step.

# Basic counts for statistics


fileCounts<-rbind(countFileData("en_US.blogs.txt"),
                  countFileData("en_US.news.txt"),
                  countFileData("en_US.twitter.txt"))


fileCounts

```




### PLOTS

plot: barra con el tamaÃ±o de los tres ficheros
```{r}

ggplot(fileCounts, aes(x=filename,weight=totalrows)) + geom_bar()

```



plot: barra con dos barras por fichero: filas y frases


```{r}


plotdata1<-data.frame(filename=fileCounts$filename, weight=fileCounts$samplerows) 
plotdata1$type <- "sample rows"
plotdata2<-data.frame(filename=fileCounts$filename, weight=fileCounts$samplesentences) 
plotdata2$type <- "sample sentences"

ggplot(rbind(plotdata1,plotdata2), aes(x=filename,weight=weight, fill=type)) + geom_bar(position="dodge")

```





```{r}

# Process execution

file1Processing<-tokenizeFile("en_US.blogs.txt")
file2Processing<-tokenizeFile("en_US.news.txt")
file3Processing<-tokenizeFile("en_US.twitter.txt")

tokens<-rbind(file1Processing,file2Processing,file3Processing)
tokens$token <- as.character(tokens$token)

save(tokens,file="./clean_data/tokens.dat")

#cleanup
rm(file1Processing,file2Processing,file3Processing)
```







```{r}
# word counts


wordCounts<-aggregate(tokens$ngram.size, by=list(filename=tokens$origin, ngram.size=tokens$ngram.size), length)
wordCounts

```


plot:

barras con words por fichero




```{r}
ggplot(wordCounts, aes(x=filename,weight=x)) + geom_bar()
```



# Word frecuencies

(indicar que se juntan  los 3 ficheros)



```{r}

frecuency<-function(ngram.size)
{
  
  df<-as.data.frame(table(tokens[tokens$ngram.size==ngram.size,]$token))
  df$ngram.size<-ngram.size
  data.frame(ngram.size=df$ngram.size,token=df$Var1,freq=df$Freq )
}


word.frequencies<-frecuency(1)
summary(word.frequencies$freq)
bigram.frequencies<-frecuency(2)
summary(bigram.frequencies$freq)
trigram.frequencies<-frecuency(3)
summary(trigram.frequencies$freq)
quatrigram.frequencies<-frecuency(4)
summary(quatrigram.frequencies$freq)


total.frequencies<-rbind(word.frequencies,bigram.frequencies,trigram.frequencies,quatrigram.frequencies)



```




Que palabras/grams son mas frecuentes

```{r}


word.frequencies[word.frequencies$freq>600,c("token","freq")]

bigram.frequencies[bigram.frequencies$freq>80,c("token","freq")]

trigram.frequencies[trigram.frequencies$freq>11,c("token","freq")]

quatrigram.frequencies[quatrigram.frequencies$freq>3,c("token","freq")]



```



Boxplot 

```{r}

ggplot(total.frequencies, aes(group=ngram.size, x=token, y=freq)) + geom_boxplot()


```





Historama de frecuencias

```{r}

total.frequencies


ggplot(total.frequencies, aes(x=freq)) + geom_histogram(binwidth = 10) +
      coord_cartesian(ylim = c(0, 10)) 



```




Historama de frecuencias que ocurren mas de 2 veces

Barra: 2 veces vs mas de 2 veces




# Exploratory data analysis

## Basic summaries
- Word counts, line counts, and basic data tables


- Basic plots - Histograms 



## Word Analysis

```{r}

str(tokens$token)

frecuencies<-table (tokens[tokens$ngram.size==1,]$token)

table(frecuencies)

#8596 unique words (appears only once)
#2247 appears twice...

hist(frecuencies)

hist(log(frecuencies))

```



