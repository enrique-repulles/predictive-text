---
title: 'Text Prediction: Exploratory data analysis'
author: "Enrique Repulles"
output:
  pdf_document:
    number_sections: yes
  html_notebook:
    number_sections: yes
  html_document:
    number_sections: yes
---

```{r}
#Installation issues

#require(devtools)
# NO install_version("tm", version = "0.6-2", repos = "http://cran.us.r-project.org")
# 0.7 no procesa bien ngrams, hay que usar  0.6-2 
# biocLite("Rgraphviz") from bioconductor

```



```{r imports, , include=FALSE}
# The libraries needed to run the code
library("NLP")
library("tm")
library("ggplot2")
library("knitr")


```


```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.width=6, fig.height=3, fig.align="center", cache = TRUE)

```



Load data and save in RDS format

```{r, eval=FALSE, include=FALSE}
sampleFileBUENO<-function(myfilename, sample.proportion)
{
  path<-paste0("./raw_data/final/en_US/",myfilename)
  out.path<-paste0("./clean_data/",strsplit(myfilename,split = ".txt")[[1]],".sampled.txt")
  raw.data = readLines(path,warn = FALSE,skipNul = TRUE);
  sample<-1==rbinom(n=length(raw.data), p=sample.proportion,size=1)
  data<-raw.data[sample]
  # saveRDS(object = data, file = out.path)
  writeLines(data,con=out.path)
  
}


sampleFile<-function(myfilename, sample.no.se.usa)
{
  path<-paste0("./raw_data/final/en_US/",myfilename)
  out.path<-paste0("./clean_data/",strsplit(myfilename,split = ".txt")[[1]],".sampled.txt")
  raw.data = readLines(path,warn = FALSE,skipNul = TRUE,n = 3);
  # saveRDS(object = data, file = out.path)
  writeLines(raw.data,con=out.path)
  
}


 sampleFile("en_US.blogs.txt", .01)
 sampleFile("en_US.news.txt", .01)
 sampleFile("en_US.twitter.txt", .01)


```




```{r}
corpus.source<-DirSource("./clean_data/")
corpus.source
corpus<-VCorpus(corpus.source, readerControl = list(reader=readPlain,language = "en"))
summary(corpus)


```


```{r}

 corpus <- tm_map(corpus, removeNumbers)
 corpus <- tm_map(corpus, removePunctuation)
 corpus <- tm_map(corpus , stripWhitespace)
# corpus <- tm_map(corpus, tolower)
# corpus <- tm_map(corpus, removeWords, stopwords("english")) # quitar
```



```{r}
#dtm <- DocumentTermMatrix(corpus)
#inspect(dtm)
```



```{r}

#Weka no funciona muy bien paralelizado
#BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
#tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
```

Tokenizing and ngram frequency calculation

```{r}

#Tokenizar trigramas

# TODO: arreglar, no saca los overlapping

#gregexpr(pattern = "[[:alpha:]]+[[:blank:]][[:alpha:]]+", "the fox jumps and fly")

unigramMatrix <- TermDocumentMatrix(corpus)

bigramtokenizer<-Regexp_Tokenizer("[[:alpha:]]+[[:blank:]][[:alpha:]]+")
bigramMatrix <- TermDocumentMatrix(corpus, control=list(tokenize=bigramtokenizer))

trigramtokenizer<-Regexp_Tokenizer("[[:alpha:]]+[[:blank:]][[:alpha:]]+[[:blank:]][[:alpha:]]+")
trigramMatrix <- TermDocumentMatrix(corpus, control=list(tokenize=trigramtokenizer))


```


Tokenizin all ngrams in one iteration
```{r}


as.vector(gregexpr(pattern="[[:blank:]]",text = "uno dos tres")[[1]])

s<-"uno dos tres"


gregexpr(text = s,pattern = " ",fixed = TRUE)

findTokens<-function(s)
{
  max.ngram<-3
  spaces <- as.vector(gregexpr(text = s,pattern = " ",fixed = TRUE)[[1]])
  spaces<-c(1,spaces,nchar(s) ) #add start and end
  print(spaces)
  from<-numeric(length(spaces)*max.ngram)
  to<-numeric(length(spaces)*max.ngram)

  breakpoints<-array(0,dim = c(length(spaces),2)) # add dim names
  words<-character(length(spaces))
  
  for (i in 1:length(spaces))
  {
    breakpoints[i,1] <-spaces[i]
    breakpoints[i,2] <-spaces[i+1]
  }

  print(breakpoints)
  
  for (i in 1:length(breakpoints))
  {
    print("Bucle")
    if (!is.na(breakpoints[i,2]))
    {
      print("Entrando")
      words[i]<-substr(s, breakpoints[i,1],breakpoints[i,2])
    }
    print(words)
  }

  words
     
}
findTokens(s)

sp <- Span_Tokenizer(f = findTokens)
test<-sp(s)
test


```


Training data construction


```{r}
totalTMD<-rbind(unigramMatrix,bigramMatrix, trigramMatrix)
attributes(totalTMD) <- attributes(bigramMatrix)
total.frequencies<-apply(as.matrix(totalTMD), FUN=sum, MARGIN=1)
# El term es el Ã­ndice

get.gram.head <- function (s)
{
  paste(s[1:length(s)-1],collapse = " ")
}


get.gram.tail <- function (s)
{
  s[length(s)]
}


test <- data.frame(
  term=names(total.frequencies),
  freq=total.frequencies,
  gram.size=sapply(strsplit(names(total.frequencies)," "), FUN =length),
  gram.head=sapply(strsplit(names(total.frequencies)," "), FUN =get.gram.head),
  gram.tail=sapply(strsplit(names(total.frequencies)," "), FUN =get.gram.tail)
)

```




- tm::c.VCorpus		Combine Corpora, Documents, Term-Document Matrices, and Term Frequency Vectors
- tm::findAssocs		Find Associations in a Term-Document Matrix
- tm::findMostFreqTerms		Find Most Frequent Terms
- tm::inspect		Inspect Objects
- tm::TermDocumentMatrix		Term-Document Matrix
- tm::plot.TermDocumentMatrix		Visualize a Term-Document Matrix
- tm::tm_term_score		Compute Score for Matching Terms
- tm::plot.TermDocumentMatrix		Visualize a Term-Document Matrix


