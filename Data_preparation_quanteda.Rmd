---
title: "Data preparation with quanteda"
output: html_notebook
---



Idea general para la predicción:

Ante una frase: 

1.	Sacar una lista de muchas palabras posibles
2.	Añadir esas palabras posibles al ngrama incompleto
3.	buscar la probabilidad de cada uno 
4.	devolver el mas probable.


Para la busqueda de posibles candidatos
- busquedas parciales en ngramas con todas las palabras del  ngrama dado
- busqueda de palabras cercanas sin stop words (palabras de “contexto”) 
- Añadir siempre los stopwords a la lista de candidatos 
 
Usar quanteda

```{r}
library("tm")
library("quanteda")
source("auxiliar_functions.R")
```



usar dfm_trim para quitar bajas frecuencias
Usar dfm_select para buscar rápido frecuencias

¿dfm_group para agrupar todos los documentos en uno?


¿fmc para contexto?




#  Training

```{r}

  # Preparation
  # Read the list of offensive words 
  offensive.words<-readLines("raw_data/Terms-to-Block.csv")
  offensive.words<-offensive.words[5:length(offensive.words)]
  offensive.words<-gsub("[\"]","",offensive.words)
  offensive.words<-gsub("[,]","",offensive.words)  

  
  # Corpus for frequency information
  corpus.source<-DirSource("./clean_data/training/")
  tmcorpus1 <- tm::VCorpus(corpus.source, readerControl = list(reader=readPlain,language = "en"))
  tmcorpus1 <- tm_map(tmcorpus1, content_transformer(char_tolower))
  # tmcorpus <- tm_map(tmcorpus, removeWords, stopwords("english")) # quitar
  tmcorpus1 <- tm_map(tmcorpus1, removeWords, offensive.words)
  tmcorpus1 <- tm_map(tmcorpus1, removeNumbers)
  tmcorpus1 <- tm_map(tmcorpus1, mark.sentences)
  #tmcorpus <- tm_map(tmcorpus, removePunctuation)

  tmcorpus1 <- tm_map(tmcorpus1 ,stripWhitespace)
  frequency.corpus<-corpus(tmcorpus1)
  

  # Ngrams frequency
  words<-tokens(frequency.corpus, remove_punct = TRUE, remove_url = TRUE)
  
  



  
  # Corpus for context information
  
  # tmcorpus2 <- tm::VCorpus(corpus.source, readerControl = list(reader=readPlain,language = "en"))
  # tmcorpus2 <- tm_map(tmcorpus2, removeNumbers)
  # tmcorpus2 <- tm_map(tmcorpus2, removePunctuation)
  # tmcorpus2 <- tm_map(tmcorpus2, content_transformer(tolower))
  # tmcorpus2 <- tm_map(tmcorpus2, removeWords, tm::stopwords("english"))
  # tmcorpus2 <- tm_map(tmcorpus2, mark.sentences)
  # tmcorpus2 <- tm_map(tmcorpus2, removeWords, offensive.words)
  # tmcorpus2 <- tm_map(tmcorpus2, stripWhitespace)
#  context.corpus<-corpus(tmcorpus2)
  
#  context.words<-tokens(context.corpus, remove_punct = TRUE, remove_url = TRUE)
  #context.bigrams<-tokens_ngrams(context.words, n = 2L)
  #context.words<-removeFeatures(x=context.words, features="EOS", valuetype="regex")
  #context.bigrams<-removeFeatures(x=context.bigrams, features="EOS", valuetype="regex")

  
#  most.frequent.words<-topfeatures(dfm(context.words),n = 100)
  
  #tmcorpus <- tm_map(tmcorpus, removeWords, most.frequent.words)
  #context.corpus<-corpus(tmcorpus)
  
  # Organize result object 
  

  calculate.ngram.freq <- function(words, n, umbral ) 
  {
    ngrams<-tokens_ngrams(words, n = n)
    ngrams<-removeFeatures(x=ngrams, features="EOS", valuetype="regex")
    ngrams.dfm<-dfm(ngrams)
    
    #word.dfm<-dfm_trim(ngrams.dfm, min_count=umbral, max_count=10)
    
    freq <- textstat_frequency(ngrams.dfm)
    ngrams.freq <-freq$frequency
    names(ngrams.freq) <- freq$feature
    
    #print(c(ngrams.freq,sum(freq$frequency),ngrams.freq/sum(freq$frequency)))
    
    ngrams.freq<-(ngrams.freq[(ngrams.freq/sum(freq$frequency))>umbral])
    ngrams.freq
  }
  
  
  #weights
  w1<-.00000001
  w2<- .01
  w3<-.6

      
  train.data<-list (
    word.freq=calculate.ngram.freq(words,1L,0 ),
    bigram.freq=calculate.ngram.freq(words,2L,2.664e-06),
    trigram.freq=calculate.ngram.freq(words,3L,2.907e-06),
    tetragram.freq=calculate.ngram.freq(words,4L,3.182e-06),
# most.frequent.words = names(most.frequent.words),
# bigram.context = context.bigrams,
    stopwords =  tm::stopwords("english"),
    weights=c(w1,w2,w3, 1-w1-w2-w3)
    )

  

```



Coberture analysis

```{r}



train.data$weights

summary(train.data$word.freq/sum(train.data$word.freq))
summary(train.data$bigram.freq/sum(train.data$bigram.freq))
summary(train.data$trigram.freq/sum(train.data$trigram.freq))
summary(train.data$tetragram.freq/sum(train.data$tetragram.freq))

```



```{r}
#cleanup
  rm (words, tmcorpus1)
  


```
)

 


# Prediction 

```{r}
str(train.data)
```




```{r}


predict.word <- function (query) 
{
  clean.query<-tolower(trimws(stripWhitespace(removePunctuation(removeNumbers(query)))))
  
  candidates<-candidates.probability(clean.query)
  
  # Remove repeated words
  
  
  list(
    candidates=candidates[order(candidates$prob,decreasing = TRUE)[1:10],], 
    word=candidates[which.max(candidates$prob),]$word
  )
  
  
}



find.candidates<-function (query)
{
  query.words<-unlist(strsplit (query," "))
  last_word <- lastn(query,1)
  candidate_pattern <- paste0(c("^","_"), last_word, "_", collapse = "|")
  candidate_index<-grep (names(train.data$bigram.freq),pattern = candidate_pattern)
  candidates<-unlist(sapply(X=names(train.data$bigram.freq)[candidate_index], FUN=function(x)lastn(x,1,"_")))
  
  #Remove candidates already in the sentence

  
  if (length(candidates)==0 & length(query.words)>1)
  { # look foor previous words
    candidates<-find.candidates(paste(query.words[1:(length(query.words)-1)],collapse = " "))
  }
  
  if (length(candidates)==0 & length(query.words)==1)
  {
    candidates<-train.data$stopwords
  }
  
  clean.candidates<-candidates[!(candidates %in% query.words)]
  

  if (length(clean.candidates)>0)
  {
    return(clean.candidates)
  }
  else 
  {
    return(candidates)
  }

    
}

candidates.probability <- function (query) {

  MAX_WORDS<-4
  

  # look for candidates

  candidates<-data.frame(word=find.candidates(query))
  candidates$full.sentence<-paste(query, candidates$word, sep = " ")

  candidates$words<-sapply(candidates$full.sentence, function (x) lastn (x,1))
  candidates$bigrams<-sapply(candidates$full.sentence, function (x) lastn (x,2))
  candidates$trigrams<-sapply(candidates$full.sentence, function (x) lastn (x,3))
  candidates$tetragrams<-sapply(candidates$full.sentence, function (x) lastn (x,4))

  #Calculate frecuencies for candidates
  
  candidates$word.freq<-train.data$word.freq[candidates$words]
  candidates$bigram.freq<-train.data$bigram.freq[candidates$bigrams]
  candidates$trigram.freq<-train.data$trigram.freq[candidates$trigrams]
  candidates$tetragram.freq<-train.data$tetragram.freq[candidates$tetragrams]

  candidates$word.freq[is.na(candidates$word.freq)]<-0
  candidates$bigram.freq[is.na(candidates$bigram.freq)]<-0
  candidates$trigram.freq[is.na(candidates$trigram.freq)]<-0
  candidates$tetragram.freq[is.na(candidates$tetragram.freq)]<-0
  candidates$word.prob<-(1+candidates$word.freq)/(1+sum(train.data$word.freq)) 
  candidates$bigram.prob<-(1+candidates$bigram.freq)/(1+sum(train.data$bigram.freq)) 
  candidates$trigram.prob<-(1+candidates$trigram.freq)/(1+sum(train.data$trigram.freq)) 
  candidates$tetragram.prob<-(1+candidates$tetragram.freq)/(1+sum(train.data$tetragram.freq)) 

  candidates$prob<-(
    train.data$weights[1]*candidates$word.prob + 
    train.data$weights[2]*candidates$bigram.prob +
    train.data$weights[3]*candidates$trigram.prob +
    train.data$weights[4]*candidates$tetragram.prob)/4
  return (candidates)

  
}


predict.word("there are many rm engines in this series some of which are new and some" )



```


```{r}

frase<-c("una","prueba","de","palabras")

candidatos<-c("la", "palabras")




```



## Save objects
```{r}


#saveRDS(object = train.data, file = "train.data.RDS")
#saveRDS(object = predict.word, file = "predict_word.RDS")

#rm(train.data,predict.word)

```







# TODO: 


- Añadir al training la búsqueda de los parámetros de backoff
- Quitar de los candidatos las palabras que ya están en la frase
- Calcular eficiencia, perplexity...
- Eliminar del contexto todo lo que termine con una palabra frecuente
- ser mas estricto al restringir los 4gramas, son lo que mas ocupa
- Eliminar nombres de persona
- Ren



