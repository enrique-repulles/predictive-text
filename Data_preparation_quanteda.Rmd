---
title: "Data preparation with quanteda"
output: html_notebook
---



Idea general para la predicción:

Ante una frase: 

1.	Sacar una lista de muchas palabras posibles
2.	Añadir esas palabras posibles al ngrama incompleto
3.	buscar la probabilidad de cada uno 
4.	devolver el mas probable.


Para la busqueda de posibles candidatos
- busquedas parciales en ngramas con todas las palabras del  ngrama dado
- busqueda de palabras cercanas sin stop words (palabras de “contexto”) 
- Añadir siempre los stopwords a la lista de candidatos 
 
Usar quanteda

```{r}
library("tm")
library("quanteda")
```



usar dfm_trim para quitar bajas frecuencias
Usar dfm_select para buscar rápido frecuencias

¿dfm_group para agrupar todos los documentos en uno?


¿fmc para contexto?



# Pruebas analisis de contexto 

```{r}

#contextWords<-tokens(context.corpus , remove_punct = TRUE, remove_url = TRUE)

contextWords <- function (corpus,word)
{
  context.before<-kwic(corpus, pattern = word,window = 2)
  # quitar las palabras frequentes, y mostrar sólo las 100 mas frecuentes de las que queden
  unique(unlist(strsplit(context.before$post," ")))  
}


```


#  Refactor funcion de Training

```{r}

training <- function (parametros="")
{
  
  # Preparation
  # Read the list of offensive words 
  offensive.words<-readLines("raw_data/Terms-to-Block.csv")
  offensive.words<-offensive.words[5:length(offensive.words)]
  offensive.words<-gsub("[\"]","",offensive.words)
  offensive.words<-gsub("[,]","",offensive.words)  


  # Corpus for frequency information
  corpus.source<-DirSource("./clean_data/")
  tmcorpus1 <- tm::VCorpus(corpus.source, readerControl = list(reader=readPlain,language = "en"))
  tmcorpus1 <- tm_map(tmcorpus1, content_transformer(char_tolower))
  # tmcorpus <- tm_map(tmcorpus, removeWords, stopwords("english")) # quitar
  #tmcorpus <- tm_map(tmcorpus, removePunctuation)
  tmcorpus1 <- tm_map(tmcorpus1, removeWords, offensive.words)
  tmcorpus1 <- tm_map(tmcorpus1, removeNumbers)

  tmcorpus1 <- tm_map(tmcorpus1 ,stripWhitespace)
  frequency.corpus<-corpus(tmcorpus1)
  

  # Ngrams frequency
  words<-tokens(frequency.corpus, remove_punct = TRUE, remove_url = TRUE)
  bigrams<-tokens_ngrams(words, n = 2L)
  trigrams<-tokens_ngrams(words, n = 3L)
  tetragrams<-tokens_ngrams(words, n = 4L)
  
  word.freq<-dfm(words)
  bigram.freq<-dfm(bigrams)
  trigram.freq<-dfm(trigrams)
  tetragram.freq<-dfm(tetragrams)
  
  
  
  
  # Corpus for context information
  
  tmcorpus2 <- tm::VCorpus(corpus.source, readerControl = list(reader=readPlain,language = "en"))
  tmcorpus2 <- tm_map(tmcorpus2, removeNumbers)
  tmcorpus2 <- tm_map(tmcorpus2, removePunctuation)
  tmcorpus2 <- tm_map(tmcorpus2, content_transformer(tolower))
  tmcorpus2 <- tm_map(tmcorpus2, removeWords, tm::stopwords("english"))
  tmcorpus2 <- tm_map(tmcorpus2, removeWords, offensive.words)
  tmcorpus2 <- tm_map(tmcorpus2, stripWhitespace)
  context.corpus<-corpus(tmcorpus2)
  
  context.words<-tokens(context.corpus, remove_punct = TRUE, remove_url = TRUE)
  most.frequent.words<-topfeatures(dfm(context.words),n = 100)

  #tmcorpus <- tm_map(tmcorpus, removeWords, most.frequent.words)
  #context.corpus<-corpus(tmcorpus)
  
  # Organize result object 
  
  list (
    word.freq = word.freq,
    bigram.freq = bigram.freq,
    trigram.freq =  trigram.freq,
    tetragram.freq =  tetragram.freq,
    stopwords =  tm::stopwords("english"),
    most.frequent.words = names(most.frequent.words),
    context.corpus = context.corpus
    )

}

```



Test de la funcion 
```{r}

fit <- training()
#str(fit)


fit$word.freq

summary(fit$word.freq)

sum(ntoken(fit$word.freq))


```



 
```{r}

ngram.probability <- function (train.data, ngram)
{
  
  print(ngram)
  tokens<-unlist(strsplit(ngram, "_"))
  # Look in bigrams
  
  bigram.pattern <- paste(tokens[1:min(2, length(tokens))], collapse = "_")
  #print(bigram.pattern)
  bigram.prob<-(1+sum(dfm_select(train.data$bigram.freq,bigram.pattern))) / (1+sum(ntoken(train.data$bigram.freq)))
  #print(bigram.prob)
  
  trigram.pattern <- paste(tokens[1:min(3, length(tokens))], collapse = "_")
  #print(trigram.pattern)
  trigram.prob<-(1+sum(dfm_select(train.data$trigram.freq,trigram.pattern))) / (1+sum(ntoken(train.data$trigram.freq)))
  #print(trigram.prob)
  
  tetragram.pattern <- paste(tokens[1:min(4, length(tokens))], collapse = "_") 
  #print(tetragram.pattern)
  tetragram.prob<-(1+sum(dfm_select(train.data$tetragram.freq,tetragram.pattern))) / (1+sum(ntoken(train.data$tetragram.freq)))
  #print(tetragram.prob)
  
  max(bigram.prob,trigram.prob, tetragram.prob)
}

#test 
ngram.probability(fit,"my_company")

```




```{r}






predict <- function (train.data, query) {

  
  MAX_WORDS=3

  clean.query<-tolower(trimws(stripWhitespace(removePunctuation(removeNumbers(query)))))
  query.words<-unlist(strsplit (clean.query," "))
  query.length<-length(query.words)
  query.words<-query.words[max(query.length-MAX_WORDS+1,1):query.length]

  # Buscar candidatos  
  context.words <- unlist(sapply(X = query.words, FUN = function (x) contextWords(fit$context.corpus,x)))  
   
  candidates<-unique(c(context.words, train.data$most.frequent.words, train.data$stopwords))
  
  # Construir candidatos
  
  candidates <- paste(paste(query.words, collapse = "_"), candidates, sep="_")

  
  # Buscar la probabilidad de cada uno 
  
  prediction <- data.frame (candidate=candidates)
  
  prediction$prob<- sapply(candidates, FUN= function (x) ngram.probability(train.data, x))
  
  
  # Construir la salida: palabra mas frecuente y mejores candidatos
  
  prediction
}


predict(fit,"fff ggg")


```






Obtencion del contexto:

Usar n-gramas, cogiendo la primera palabra del ngrama como key y el resto como valor, (o la última para el contexto anterior)

Método: 
Para cada n-grama "anterior" (4-gramas)
sacar la última palabra
Si es stopword o simbolo ignorar 

Para cada palabra, añadir las palabras anteriores  bajo la kery de la última

Lo mismo para n-gramas "posteriores" (2-gramas)  

Añadir: 
  añadir sin repetidos
  no se añaden stop word


En el corpus de "contexto", quitar las palabras más frecuentes del corpus, y añadirlas a parte como candidatos


(¿usar librería hashmap?)



¿No procesarolo por ngramas sino directamente el fichero? 

¿buscar simplemente los ngramas donde aparece la palabra? ¿diccionarios?

# TODO: 
Que al guardar los objetos de digramas, ngramas etc los guarde ya con documentos unificados
Añadir al training la búsqueda de los parámetros de backoff


¿Hacer corpus_reshape(corp, to = "sentences") para contexto?
No guardar frecuencias por documentos, sino agrupar las frecuencias de cada token