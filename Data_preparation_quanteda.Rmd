---
title: "Data preparation with quanteda"
output: html_notebook
---



Idea general para la predicción:

Ante una frase: 

1.	Sacar una lista de muchas palabras posibles
2.	Añadir esas palabras posibles al ngrama incompleto
3.	buscar la probabilidad de cada uno 
4.	devolver el mas probable.


Para la busqueda de posibles candidatos
- busquedas parciales en ngramas con todas las palabras del  ngrama dado
- busqueda de palabras cercanas sin stop words (palabras de “contexto”) 
- Añadir siempre los stopwords a la lista de candidatos 
 
Usar quanteda

```{r}
library("tm")
library("quanteda")
```



usar dfm_trim para quitar bajas frecuencias
Usar dfm_select para buscar rápido frecuencias

¿dfm_group para agrupar todos los documentos en uno?


¿fmc para contexto?




#  Training

```{r}

  # Preparation
  # Read the list of offensive words 
  offensive.words<-readLines("raw_data/Terms-to-Block.csv")
  offensive.words<-offensive.words[5:length(offensive.words)]
  offensive.words<-gsub("[\"]","",offensive.words)
  offensive.words<-gsub("[,]","",offensive.words)  


  # Corpus for frequency information
  corpus.source<-DirSource("./clean_data/training/")
  tmcorpus1 <- tm::VCorpus(corpus.source, readerControl = list(reader=readPlain,language = "en"))
  tmcorpus1 <- tm_map(tmcorpus1, content_transformer(char_tolower))
  # tmcorpus <- tm_map(tmcorpus, removeWords, stopwords("english")) # quitar
  #tmcorpus <- tm_map(tmcorpus, removePunctuation)
  tmcorpus1 <- tm_map(tmcorpus1, removeWords, offensive.words)
  tmcorpus1 <- tm_map(tmcorpus1, removeNumbers)

  tmcorpus1 <- tm_map(tmcorpus1 ,stripWhitespace)
  frequency.corpus<-corpus(tmcorpus1)
  

  # Ngrams frequency
  words<-tokens(frequency.corpus, remove_punct = TRUE, remove_url = TRUE)
  bigrams<-tokens_ngrams(words, n = 2L)
  trigrams<-tokens_ngrams(words, n = 3L)
  tetragrams<-tokens_ngrams(words, n = 4L)
  
  word.freq<-dfm(words)
  bigram.freq<-dfm(bigrams)
  trigram.freq<-dfm(trigrams)
  tetragram.freq<-dfm(tetragrams)
  

  
  # Corpus for context information
  
  tmcorpus2 <- tm::VCorpus(corpus.source, readerControl = list(reader=readPlain,language = "en"))
  tmcorpus2 <- tm_map(tmcorpus2, removeNumbers)
  tmcorpus2 <- tm_map(tmcorpus2, removePunctuation)
  tmcorpus2 <- tm_map(tmcorpus2, content_transformer(tolower))
  tmcorpus2 <- tm_map(tmcorpus2, removeWords, tm::stopwords("english"))
  tmcorpus2 <- tm_map(tmcorpus2, removeWords, offensive.words)
  tmcorpus2 <- tm_map(tmcorpus2, stripWhitespace)
  context.corpus<-corpus(tmcorpus2)
  
  context.words<-tokens(context.corpus, remove_punct = TRUE, remove_url = TRUE)
  most.frequent.words<-topfeatures(dfm(context.words),n = 100)
  context.bigrams<-dfm(tokens_ngrams(context.words, n = 2L))
  
  
  
  
  #tmcorpus <- tm_map(tmcorpus, removeWords, most.frequent.words)
  #context.corpus<-corpus(tmcorpus)
  
  # Organize result object 

  
  
  
  train.data<-list (
    word.freq = docfreq(word.freq, scheme="count" )  ,
    bigram.freq = docfreq(bigram.freq, scheme="count" )  ,
    trigram.freq =  docfreq(trigram.freq, scheme="count" )  ,
    tetragram.freq =  docfreq(tetragram.freq, scheme="count" )  ,
    stopwords =  tm::stopwords("english"),
    most.frequent.words = names(most.frequent.words),
    bigram.context = featnames(context.bigrams)
    
    )

  

```

```{r}
#cleanup
  rm (word.freq,  bigram.freq,trigram.freq,tetragram.freq,most.frequent.words,context.corpus, tmcorpus1, tmcorpus2)
  rm (bigrams, trigrams, tetragrams, context.bigrams, context.words, words)


```



 
```{r}

ngram.probability <- function (train.data, ngram)
{
  
  print(ngram)
  tokens<-unlist(strsplit(ngram, "_"))
  # Look in bigrams
  
  bigram.pattern <- paste(tokens[1:min(2, length(tokens))], collapse = "_")
  #print(bigram.pattern)
  bigram.prob<-(1+sum(dfm_select(train.data$bigram.freq,bigram.pattern))) / (1+sum(ntoken(train.data$bigram.freq)))
  #print(bigram.prob)
  
  trigram.pattern <- paste(tokens[1:min(3, length(tokens))], collapse = "_")
  #print(trigram.pattern)
  trigram.prob<-(1+sum(dfm_select(train.data$trigram.freq,trigram.pattern))) / (1+sum(ntoken(train.data$trigram.freq)))
  #print(trigram.prob)
  
  tetragram.pattern <- paste(tokens[1:min(4, length(tokens))], collapse = "_") 
  #print(tetragram.pattern)
  tetragram.prob<-(1+sum(dfm_select(train.data$tetragram.freq,tetragram.pattern))) / (1+sum(ntoken(train.data$tetragram.freq)))
  #print(tetragram.prob)
  
  max(bigram.prob,trigram.prob, tetragram.prob)
}

#test 
#ngram.probability(training.data,"my_company")

```



#auxiliar function 

Last n words of a string
```{r}
lastn<-function (str, n, sep=" ")
{
  
  v <- unlist(strsplit(str, sep))
  from <- max(1,length(v)+1-n)
  to <- length(v)
  paste(v[from:to], collapse = "_")
}
# test
# lastn("uno dos tres", 3)
```



# Prediction 



```{r}



predict <- function (query) {

MAX_WORDS=3

clean.query<-tolower(trimws(stripWhitespace(removePunctuation(removeNumbers(query)))))
query.words<-unlist(strsplit (clean.query," "))


# Buscar candidatos de cada una de las palabras de la query

context<-train.data$bigram.context
context.words<-unlist(sapply(X=query.words, FUN=function (x) context[grep(x=context,pattern=paste0("^",x,"_"))]))
context.words<-sapply(X=context.words, FUN = function (x) lastn (x,1, sep="_"))


candidates<-unique(c(context.words, train.data$most.frequent.words, train.data$stopwords))
candidates<-data.frame(word=candidates)
candidates$full.sentence<-paste(clean.query, candidates$word, sep = " ")


candidates$bigrams<-sapply(candidates$full.sentence, function (x) lastn (x,2))
candidates$trigrams<-sapply(candidates$full.sentence, function (x) lastn (x,3))
candidates$tetragrams<-sapply(candidates$full.sentence, function (x) lastn (x,4))

candidates$bigram.freq<-train.data$bigram.freq[candidates$bigrams]
candidates$trigram.freq<-train.data$trigram.freq[candidates$trigram]
candidates$tetragram.freq<-train.data$tetragram.freq[candidates$tetragram]

candidates$bigram.freq[is.na(candidates$bigram.freq)]<-0
candidates$trigram.freq[is.na(candidates$trigram.freq)]<-0
candidates$tetragram.freq[is.na(candidates$tetragram.freq)]<-0

candidates$bigram.prob<-(1+candidates$bigram.freq)/(1+sum(train.data$bigram.freq))
candidates$trigram.prob<-(1+candidates$trigram.freq)/(1+sum(train.data$trigram.freq))
candidates$tetragram.prob<-(1+candidates$tetragram.freq)/(1+sum(train.data$tetragram.freq))

#WEIGHTS
w1 <- .1
w2 <- .3
w3 <- .6

candidates$prob<-(w1*candidates$bigram.prob+w2*candidates$trigram.prob+w3*candidates$tetragram.prob)/3


######

#Result


candidates[order(candidates$prob,decreasing = TRUE)[1:10],c("full.sentence","prob")]

}

predict("tomorrow I will")


```








# TODO: 


- Añadir al training la búsqueda de los parámetros de backoff
- ¿Hacer corpus_reshape(corp, to = "sentences") para contexto?
- No guardar frecuencias por documentos, sino agrupar las frecuencias de cada token
- Hacer clustering para palabras asociadas
- Quitar de los candidatos las palabras que ya están en la frase
- Calcular eficiencia, perplexity...
- Marcar los finales de frase con EOS para no contraminar contexto de una frase con el siguiente ( y quitar de los candidatos)
- en vez de clean_data, crear subcarpetas de training y test


